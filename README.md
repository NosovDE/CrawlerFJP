# CrawlerFJP
Crawler with ForkJoinPool class
Веб краулер с использованием класса ForkJoinPool для распараллеливания процесса обхода сайта.
Данные о доменах считываются из файла, на каждом домене краулер посещает не более 100 страниц, между запросами к одному сайту делается пауза в 1 секунду.
Обработка производится в 128 потоков по умолчанию.
После окончания работы все уникальные ссылки записываются в файл result.txt.

# Требования:
 - jdk 1.8+
 - maven 3+

# Сборка проекта: 
  - распаковать исходники проекта
  - в корне проекта (папка source) выполнить команду: mvn clean assembly:assembly
  - в директории target будет готовый для запуска crawler.jar и файл 100DomainsForCrawling.txt с доменами.

# Инструкция по запуску:
  - Запуск краулера с кол-вом потоков 128 (по умолчанию):  localhost>  java -jar crawler.jar  
  - Возможные параметры для запуска: localhost>  java -jar crawler.jar {кол-во потоков в пуле} {признак обработки robots.txt}
      Пример с запуском пула в 256 потоков:  localhost>  java -jar crawler.jar 256
      Пример с запуском пула в 64 потока и обработкой robots.txt:  localhost>  java -jar crawler.jar 64 allow

# Дополнительно
Обработка robots.txt не сделана до конца. Производится считывание страниц для исключения их из обхода, но правила не применяются.
По тестам на машине с 8 ядрами (4-ре хардварных), самое оптимальное время работы краулера получается при 128 потоках и составляет,  приблизительно 3 минуты.

Программа пишет всю необходимую информацию в лог, после запуска, в директории где производится запуск, появляется директория logs с логами.
После окончания работы программа выводит в лог информацию, какие внутренние страницы домена были посещены. Производится подсчет времени работы программы и результат собранных ссылок записываются в файл result.txt, в директории откуда была запущена программа
